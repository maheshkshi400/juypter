{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---+------+-------+------+\n",
      "| id|   name|gender|age|salary|   dept|deptno|\n",
      "+---+-------+------+---+------+-------+------+\n",
      "|  1| mahesh|  male| 20|  5000|     IT|    50|\n",
      "|  2|shruthi|female| 30|  6000|     HR|    20|\n",
      "|  3| ganesh|  male| 25|  7000|FINANCE|    30|\n",
      "|  4|  pooja|female| 34|  8000|PAYROLL|    40|\n",
      "|  5| kamini|female| 31|  6000|     IT|    50|\n",
      "+---+-------+------+---+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"https://github.com/maheshkshi400/MK\").getOrCreate()\n",
    "\n",
    "data1 = [(1,'mahesh','male',20,5000,'IT',50),\n",
    "         (2,'shruthi','female',30,6000,'HR',20),\n",
    "         (3,'ganesh','male',25,7000,'FINANCE',30),\n",
    "         (4,'pooja','female',34,8000,'PAYROLL',40),\n",
    "         (5,'kamini','female',31,6000,'IT',50)]\n",
    "\n",
    "schema1 = ['id','name','gender','age','salary','dept','deptno']\n",
    "\n",
    "df1 = spark.createDataFrame(data1,schema1)\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|company|Quarter|Revenue|\n",
      "+-------+-------+-------+\n",
      "|    ABC|     Q1|  60000|\n",
      "|    XYZ|     Q1|  45000|\n",
      "|    JKL|     Q2|  80000|\n",
      "|    STU|     Q3|  75000|\n",
      "|    ABC|     Q2|  60000|\n",
      "|    ERS|     Q4|  55000|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2= [('ABC','Q1',60000),\n",
    "        ('XYZ','Q1',45000),\n",
    "        ('JKL','Q2',80000),\n",
    "        ('STU','Q3',75000),\n",
    "        ('ABC','Q2',60000),\n",
    "        ('ERS','Q4',55000)]\n",
    "\n",
    "schema=['company','Quarter','Revenue']\n",
    "\n",
    "df2 = spark.createDataFrame(data2,schema)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+-----+\n",
      "|company|   Q1|   Q2|   Q3|   Q4|\n",
      "+-------+-----+-----+-----+-----+\n",
      "|    ERS| NULL| NULL| NULL|55000|\n",
      "|    JKL| NULL|80000| NULL| NULL|\n",
      "|    STU| NULL| NULL|75000| NULL|\n",
      "|    XYZ|45000| NULL| NULL| NULL|\n",
      "|    ABC|60000|60000| NULL| NULL|\n",
      "+-------+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df3= df2.groupBy('company').pivot('Quarter').sum('Revenue')\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|company|Quarter|Revenue|\n",
      "+-------+-------+-------+\n",
      "|    ERS|     Q1|   NULL|\n",
      "|    ERS|     Q2|   NULL|\n",
      "|    ERS|     Q3|   NULL|\n",
      "|    ERS|     Q4|  55000|\n",
      "|    JKL|     Q1|   NULL|\n",
      "|    JKL|     Q2|  80000|\n",
      "|    JKL|     Q3|   NULL|\n",
      "|    JKL|     Q4|   NULL|\n",
      "|    STU|     Q1|   NULL|\n",
      "|    STU|     Q2|   NULL|\n",
      "|    STU|     Q3|  75000|\n",
      "|    STU|     Q4|   NULL|\n",
      "|    XYZ|     Q1|  45000|\n",
      "|    XYZ|     Q2|   NULL|\n",
      "|    XYZ|     Q3|   NULL|\n",
      "|    XYZ|     Q4|   NULL|\n",
      "|    ABC|     Q1|  60000|\n",
      "|    ABC|     Q2|  60000|\n",
      "|    ABC|     Q3|   NULL|\n",
      "|    ABC|     Q4|   NULL|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df4= df3.select('company',expr(\"stack(4,'Q1',Q1,'Q2',Q2,'Q3',Q3,'Q4',Q4) as (Quarter,Revenue)\"))\n",
    "df4.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
