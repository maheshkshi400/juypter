{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-A-12-2-B-23-3-C-34-4-D-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba36b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.put(\"dbfs:/FileStore/interview_1.txt\",'1-A-12-2-B-23-3-C-34-4-D-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939463ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"dbfs:/FileStore/interview_1.txt\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a189b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,explode,split\n",
    "df1 = df.withColumn(\"new_value\",regexp_replace(\"value\",\"(.*?\\\\-){3}\",\"$0,\")).drop(\"value\")\n",
    "df1.show(truncate=False)\n",
    "df2 = df1.withColumn(\"new_value_1\",explode(split(df1.new_value,'-,'))).drop(\"new_value\")\n",
    "df2.show()\n",
    "df3 = df2.select(split(df2.new_value_1,'-'))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763050a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "\n",
    "df4 = df2.withColumn(\"ID\", split(col(\"new_value_1\"), \"-\")[0]).withColumn(\"NAME\", split(col(\"new_value_1\"), \"-\")[1]).withColumn(\"AGE\", split(col(\"new_value_1\"), \"-\")[2]).drop(\"new_value_1\")\n",
    "\n",
    "df4.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
